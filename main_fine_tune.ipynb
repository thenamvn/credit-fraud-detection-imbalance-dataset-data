{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf69ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (f1_score, precision_score, recall_score, \n",
    "                             roc_auc_score, precision_recall_curve, auc, \n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Boosting Models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ADVANCED FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Tạo các features tập trung vào sự bất thường trong hành vi tiêu dùng\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- 1. Xử lý thời gian ---\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    \n",
    "    df['hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "    df['month'] = df['trans_date_trans_time'].dt.month\n",
    "    \n",
    "    # Tuổi khách hàng\n",
    "    df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days // 365\n",
    "    \n",
    "    # --- 2. Khoảng cách địa lý ---\n",
    "    df['distance_km'] = np.sqrt(\n",
    "        (df['lat'] - df['merch_lat'])**2 + (df['long'] - df['merch_long'])**2\n",
    "    ) * 111\n",
    "    \n",
    "    # --- 3. Transaction Aggregations (NO LEAKAGE) ---\n",
    "    df = df.sort_values(['trans_date_trans_time'])\n",
    "    \n",
    "    df['card_mean_amt'] = df.groupby('cc_num')['amt'].expanding().mean().reset_index(level=0, drop=True)\n",
    "    df['card_std_amt'] = df.groupby('cc_num')['amt'].expanding().std().reset_index(level=0, drop=True)\n",
    "    df['card_trans_count'] = df.groupby('cc_num')['amt'].expanding().count().reset_index(level=0, drop=True)\n",
    "    df['category_mean_amt'] = df.groupby('category')['amt'].expanding().mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    df['card_std_amt'] = df['card_std_amt'].fillna(0)\n",
    "    df['amt_zscore'] = (df['amt'] - df['card_mean_amt']) / (df['card_std_amt'] + 1e-5)\n",
    "    df['amt_vs_category_mean'] = df['amt'] / (df['category_mean_amt'] + 1e-5)\n",
    "    \n",
    "    # --- 4. Log transform ---\n",
    "    df['amt_log'] = np.log1p(df['amt'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# 2. PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_data(df, label_encoders=None, is_train=True):\n",
    "    df = create_features(df)\n",
    "    \n",
    "    drop_cols = ['trans_date_trans_time', 'cc_num', 'merchant', 'first', 'last', \n",
    "                 'street', 'city', 'state', 'zip', 'dob', 'trans_num', 'unix_time',\n",
    "                 'lat', 'long', 'merch_lat', 'merch_long', 'Unnamed: 0'] \n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "    \n",
    "    cat_cols = ['category', 'gender', 'job']\n",
    "    \n",
    "    if is_train:\n",
    "        label_encoders = {}\n",
    "        for col in cat_cols:\n",
    "            if col in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "    else:\n",
    "        for col in cat_cols:\n",
    "            if col in df.columns and col in label_encoders:\n",
    "                df[col] = df[col].map(lambda x: x if x in label_encoders[col].classes_ else 'Unknown')\n",
    "                if 'Unknown' not in label_encoders[col].classes_:\n",
    "                    label_encoders[col].classes_ = np.append(label_encoders[col].classes_, 'Unknown')\n",
    "                df[col] = label_encoders[col].transform(df[col].astype(str))\n",
    "    \n",
    "    import re\n",
    "    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    \n",
    "    return df, label_encoders\n",
    "\n",
    "# ============================================================================\n",
    "# 3. THRESHOLD OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def find_optimal_threshold(y_true, y_proba):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "    \n",
    "    print(f\"\\n>>> Optimal Threshold Found: {best_threshold:.4f}\")\n",
    "    print(f\">>> Best F1-Score at threshold: {best_f1:.4f}\")\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "# ============================================================================\n",
    "# 4. HYPERPARAMETER TUNING với OPTUNA\n",
    "# ============================================================================\n",
    "\n",
    "def objective_xgb(trial, X_train, y_train, scale_weight):\n",
    "    \"\"\"Objective function cho XGBoost\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'scale_pos_weight': scale_weight,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, verbose=False)\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Tìm threshold tối ưu cho fold này\n",
    "        threshold = find_optimal_threshold(y_val, y_pred_proba)\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "def objective_lgb(trial, X_train, y_train, scale_weight):\n",
    "    \"\"\"Objective function cho LightGBM\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'scale_pos_weight': scale_weight,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        threshold = find_optimal_threshold(y_val, y_pred_proba)\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "def objective_cat(trial, X_train, y_train):\n",
    "    \"\"\"Objective function cho CatBoost\"\"\"\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'auto_class_weights': 'Balanced',\n",
    "        'random_state': 42,\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        threshold = find_optimal_threshold(y_val, y_pred_proba)\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "def tune_models(X_train, y_train, scale_weight, n_trials=50):\n",
    "    \"\"\"\n",
    "    Tối ưu hóa hyperparameters cho cả 3 mô hình\n",
    "    \n",
    "    Args:\n",
    "        n_trials: Số lần thử nghiệm cho mỗi model (càng cao càng tốt nhưng chậm hơn)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HYPERPARAMETER TUNING WITH OPTUNA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Tắt log của Optuna để giảm nhiễu\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    \n",
    "    best_params = {}\n",
    "    \n",
    "    # --- 1. Tune XGBoost ---\n",
    "    print(\"\\n[1/3] Tuning XGBoost...\")\n",
    "    study_xgb = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=TPESampler(seed=42)\n",
    "    )\n",
    "    study_xgb.optimize(\n",
    "        lambda trial: objective_xgb(trial, X_train, y_train, scale_weight),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    best_params['xgb'] = study_xgb.best_params\n",
    "    print(f\"Best XGBoost F1-Score: {study_xgb.best_value:.4f}\")\n",
    "    print(f\"Best Params: {study_xgb.best_params}\")\n",
    "    \n",
    "    # --- 2. Tune LightGBM ---\n",
    "    print(\"\\n[2/3] Tuning LightGBM...\")\n",
    "    study_lgb = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=TPESampler(seed=42)\n",
    "    )\n",
    "    study_lgb.optimize(\n",
    "        lambda trial: objective_lgb(trial, X_train, y_train, scale_weight),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    best_params['lgb'] = study_lgb.best_params\n",
    "    print(f\"Best LightGBM F1-Score: {study_lgb.best_value:.4f}\")\n",
    "    print(f\"Best Params: {study_lgb.best_params}\")\n",
    "    \n",
    "    # --- 3. Tune CatBoost ---\n",
    "    print(\"\\n[3/3] Tuning CatBoost...\")\n",
    "    study_cat = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=TPESampler(seed=42)\n",
    "    )\n",
    "    study_cat.optimize(\n",
    "        lambda trial: objective_cat(trial, X_train, y_train),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    best_params['cat'] = study_cat.best_params\n",
    "    print(f\"Best CatBoost F1-Score: {study_cat.best_value:.4f}\")\n",
    "    print(f\"Best Params: {study_cat.best_params}\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "# ============================================================================\n",
    "# 5. MODEL TRAINING với BEST PARAMS\n",
    "# ============================================================================\n",
    "\n",
    "def train_ensemble_model_tuned(X_train, y_train, X_test, y_test, best_params, scale_weight):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING ENSEMBLE MODEL WITH TUNED HYPERPARAMETERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # --- 1. Build Models với Best Params ---\n",
    "    clf_xgb = xgb.XGBClassifier(\n",
    "        **best_params['xgb'],\n",
    "        scale_pos_weight=scale_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    clf_lgb = lgb.LGBMClassifier(\n",
    "        **best_params['lgb'],\n",
    "        scale_pos_weight=scale_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    clf_cat = CatBoostClassifier(\n",
    "        **best_params['cat'],\n",
    "        auto_class_weights='Balanced',\n",
    "        random_state=42,\n",
    "        verbose=0,\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "\n",
    "    # --- 2. Ensemble ---\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('xgb', clf_xgb),\n",
    "            ('lgb', clf_lgb),\n",
    "            ('cat', clf_cat)\n",
    "        ],\n",
    "        voting='soft',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # --- 3. Training ---\n",
    "    print(\"Fitting Ensemble Model...\")\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    # --- 4. Prediction ---\n",
    "    print(\"Predicting probabilities...\")\n",
    "    y_pred_proba = ensemble.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return ensemble, y_pred_proba\n",
    "\n",
    "# ============================================================================\n",
    "# 6. EVALUATION & VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_performance(y_test, y_pred_proba, threshold):\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL EVALUATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC Score:  {pr_auc:.4f}\")\n",
    "    \n",
    "    # --- Visualization ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title(f'Confusion Matrix (Threshold={threshold:.3f})')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    \n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    axes[1].plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    axes[2].plot(recall, precision, label=f'PR-AUC = {pr_auc:.3f}', color='green')\n",
    "    axes[2].set_title('Precision-Recall Curve')\n",
    "    axes[2].set_xlabel('Recall')\n",
    "    axes[2].set_ylabel('Precision')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        df = pd.read_csv('fraudTrain.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the path.\")\n",
    "        from sklearn.datasets import make_classification\n",
    "        X_dummy, y_dummy = make_classification(n_samples=10000, n_features=20, weights=[0.99, 0.01], random_state=42)\n",
    "        df = pd.DataFrame(X_dummy, columns=[f'col_{i}' for i in range(20)])\n",
    "        df['is_fraud'] = y_dummy\n",
    "        df['trans_date_trans_time'] = pd.Timestamp('2023-01-01')\n",
    "        df['dob'] = pd.Timestamp('1990-01-01')\n",
    "        df['lat'] = 0; df['long'] = 0; df['merch_lat'] = 1; df['merch_long'] = 1\n",
    "        df['cc_num'] = 123; df['amt'] = 100; df['category'] = 'misc'\n",
    "        print(\"Using Dummy Data for demonstration...\")\n",
    "\n",
    "    # 2. Preprocessing\n",
    "    print(\"Preprocessing & Feature Engineering...\")\n",
    "    df_processed, label_encoders = preprocess_data(df, is_train=True)\n",
    "    \n",
    "    X = df_processed.drop('is_fraud', axis=1)\n",
    "    y = df_processed['is_fraud']\n",
    "    \n",
    "    # 3. Split Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    print(f\"Fraud ratio in Train: {y_train.mean():.4%}\")\n",
    "    \n",
    "    # Calculate scale_pos_weight\n",
    "    neg_count = len(y_train) - y_train.sum()\n",
    "    pos_count = y_train.sum()\n",
    "    scale_weight = neg_count / pos_count\n",
    "    print(f\"Calculated scale_pos_weight: {scale_weight:.2f}\")\n",
    "    \n",
    "    # 4. HYPERPARAMETER TUNING\n",
    "    # Thay đổi n_trials nếu muốn tăng/giảm thời gian tuning\n",
    "    # n_trials=50 ~ 30-60 phút tùy thuộc vào dataset size\n",
    "    best_params = tune_models(X_train, y_train, scale_weight, n_trials=30)\n",
    "    \n",
    "    # Lưu best params để tái sử dụng\n",
    "    joblib.dump(best_params, 'best_hyperparameters.pkl')\n",
    "    print(\"\\nBest hyperparameters saved to: best_hyperparameters.pkl\")\n",
    "    \n",
    "    # 5. Train Ensemble với Best Params\n",
    "    model, y_pred_proba = train_ensemble_model_tuned(\n",
    "        X_train, y_train, X_test, y_test, best_params, scale_weight\n",
    "    )\n",
    "    \n",
    "    # 6. Find Optimal Threshold\n",
    "    optimal_threshold = find_optimal_threshold(y_test, y_pred_proba)\n",
    "    \n",
    "    # 7. Evaluate\n",
    "    evaluate_performance(y_test, y_pred_proba, threshold=optimal_threshold)\n",
    "\n",
    "    # 8. SAVE MODEL & CONFIGS\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVING MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model_filename = 'fraud_detection_model_tuned.pkl'\n",
    "    save_payload = {\n",
    "        'model': model,\n",
    "        'threshold': optimal_threshold,\n",
    "        'features': X_train.columns.tolist(),\n",
    "        'label_encoders': label_encoders,\n",
    "        'best_params': best_params  # Lưu luôn best params để tham khảo\n",
    "    }\n",
    "    joblib.dump(save_payload, model_filename)\n",
    "    print(f\"Tuned model saved to: {model_filename}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETED!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 8. TEST ON NEW FILE (fraudTest.csv)\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING ON NEW FILE (fraudTest.csv)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_file_path = 'fraudTest.csv'\n",
    "\n",
    "try:\n",
    "    # 8.1 Load Test Data\n",
    "    print(f\"Loading test data from {test_file_path}...\")\n",
    "    df_new_test = pd.read_csv(test_file_path)\n",
    "    \n",
    "    # 8.2 Preprocess Test Data\n",
    "    # Lưu ý: Ta tái sử dụng hàm preprocess_data. \n",
    "    # Trong môi trường Production nghiêm ngặt, bạn nên lưu LabelEncoder từ bước train \n",
    "    # để transform cho test. Ở đây ta giả định các category (Gender, Job...) tương đồng.\n",
    "    print(\"Preprocessing test data...\")\n",
    "    df_new_processed, _ = preprocess_data(df_new_test, is_train=False, label_encoders=label_encoders)\n",
    "    \n",
    "    # Tách X, y\n",
    "    if 'is_fraud' in df_new_processed.columns:\n",
    "        X_new = df_new_processed.drop('is_fraud', axis=1)\n",
    "        y_new = df_new_processed['is_fraud']\n",
    "    else:\n",
    "        X_new = df_new_processed\n",
    "        y_new = None\n",
    "        print(\"Note: 'is_fraud' column not found in test data. Skipping evaluation metrics.\")\n",
    "    \n",
    "    # Đảm bảo thứ tự cột giống hệt lúc train (quan trọng cho XGBoost/LightGBM)\n",
    "    # Nếu thiếu cột nào (do drop), code sẽ báo lỗi hoặc cần xử lý thêm\n",
    "    X_new = X_new[X_train.columns]\n",
    "    \n",
    "    # 8.3 Predict using Trained Model\n",
    "    print(\"Predicting on new data...\")\n",
    "    y_new_proba = model.predict_proba(X_new)[:, 1]\n",
    "    \n",
    "    # 8.4 Evaluate using the OPTIMAL THRESHOLD found earlier\n",
    "    if y_new is not None:\n",
    "        print(f\"Evaluating using saved threshold: {optimal_threshold:.4f}\")\n",
    "        evaluate_performance(y_new, y_new_proba, threshold=optimal_threshold)\n",
    "    else:\n",
    "        print(\"Predictions generated (y_new_proba).\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: File {test_file_path} not found. Skipping external test.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311db96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Trực quan hóa tầm quan trọng của các đặc trưng từ VotingClassifier\n",
    "    bằng cách lấy trung bình từ các model con.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Lấy danh sách các model con đã được train\n",
    "    estimators = model.estimators_\n",
    "    \n",
    "    importances_list = []\n",
    "    \n",
    "    for clf in estimators:\n",
    "        # Kiểm tra xem model có thuộc tính feature_importances_ không\n",
    "        if hasattr(clf, 'feature_importances_'):\n",
    "            imp = clf.feature_importances_\n",
    "            # Chuẩn hóa về thang đo 0-1 (vì CatBoost, LGBM có thể có thang đo khác nhau)\n",
    "            if imp.sum() > 0:\n",
    "                imp = imp / imp.sum()\n",
    "            importances_list.append(imp)\n",
    "            \n",
    "    if not importances_list:\n",
    "        print(\"Không thể trích xuất feature importance từ các model con.\")\n",
    "        return\n",
    "\n",
    "    # Tính trung bình cộng độ quan trọng từ 3 model\n",
    "    avg_importance = np.mean(importances_list, axis=0)\n",
    "    \n",
    "    # Tạo DataFrame để hiển thị\n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': avg_importance\n",
    "    })\n",
    "    \n",
    "    # Sắp xếp giảm dần\n",
    "    fi_df = fi_df.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    # In ra Top 10\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(fi_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Vẽ biểu đồ\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=fi_df.head(20), palette='viridis')\n",
    "    plt.title('Top 20 Feature Importance (Ensemble Average)')\n",
    "    plt.xlabel('Normalized Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(model, X_train.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-9-21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
